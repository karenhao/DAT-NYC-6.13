{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Six Degrees of Kevin Bacon\n",
    "\n",
    "![](images/Kevin_Bacon.jpg)\n",
    "\n",
    "This activity is motivated by the text **Web Scraping with Python** by Ryan Mitchell, available through O'Reilly [here](http://shop.oreilly.com/product/0636920078067.do).  This book goes in depth with much more on using different libraries with Python around common webscraping tasks and I highly recommend it.  We will focus on the activity of moving from a base page to further pages through their links.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Links\n",
    "\n",
    "Below, we take the page dealing with the six degrees of Keving Bacon problem.  Here, our goal is to extract links to other pages that we will subsequently pass to requests.  Recall that a link is located in an `<a>` tag and the link is contained in the `href` attribute.  For example, the tag\n",
    "\n",
    "```HTML\n",
    "<a href=\"/wiki/Six_degrees_of_separation\" title=\"Six degrees of separation\">six degrees of separation</a>\n",
    "```\n",
    "\n",
    "references the Six Degrees of Separation article.  Note that this is a url within Wikipedia.  We can isolate these inner Wikipedia references.  To begin, let's inspect the link content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#mw-head\n",
      "#p-search\n",
      "/wiki/File:Kevin_Bacon.jpg\n",
      "/wiki/File:Kevin_Bacon.jpg\n",
      "/wiki/Parlor_game\n",
      "/wiki/Six_degrees_of_separation\n",
      "/wiki/Kevin_Bacon\n",
      "/wiki/Hollywood\n",
      "/wiki/Kevin_Bacon\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a')[:10]:\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, seems there are links outside of the inner wiki links.  However, we see that the wiki links contain `/wiki/`, no colons, and the links are all within the body of the page.  Exploiting these means we can write a regular expression \n",
    "\n",
    "```\n",
    "^(/wiki/)((?!:).)*$\n",
    "```\n",
    "\n",
    "that will match only the wiki links.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/Parlor_game\n",
      "/wiki/Six_degrees_of_separation\n",
      "/wiki/Kevin_Bacon\n",
      "/wiki/Hollywood\n",
      "/wiki/Kevin_Bacon\n",
      "/wiki/Kevin_Bacon\n",
      "/wiki/Charitable_organization\n",
      "/wiki/SixDegrees.org\n",
      "/wiki/Premiere_(magazine)\n",
      "/wiki/Kevin_Bacon\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find('div', {'id': 'bodyContent'}).find_all('a', href = re.compile('^(/wiki/)((?!:).)*$'))[:10]:\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Function for Links\n",
    "\n",
    "Now, let's write a function that extracts the link from any wikipedia page.  We should be able to use the idea that the links we care about are located in the same place as our Six Degrees example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikilinks(url):\n",
    "    links = []\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for link in soup.find('div', {'id': 'bodyContent'}).find_all('a', href = re.compile('^(/wiki/)((?!:).)*$')):\n",
    "        links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = get_wikilinks('https://en.wikipedia.org/wiki/Kevin_Bacon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"mw-disambig\" href=\"/wiki/Kevin_Bacon_(disambiguation)\" title=\"Kevin Bacon (disambiguation)\">Kevin Bacon (disambiguation)</a>,\n",
       " <a href=\"/wiki/San_Diego_Comic-Con\" title=\"San Diego Comic-Con\">San Diego Comic-Con</a>,\n",
       " <a href=\"/wiki/Philadelphia\" title=\"Philadelphia\">Philadelphia</a>,\n",
       " <a href=\"/wiki/Pennsylvania\" title=\"Pennsylvania\">Pennsylvania</a>,\n",
       " <a href=\"/wiki/Kyra_Sedgwick\" title=\"Kyra Sedgwick\">Kyra Sedgwick</a>,\n",
       " <a href=\"/wiki/Sosie_Bacon\" title=\"Sosie Bacon\">Sosie Bacon</a>,\n",
       " <a href=\"/wiki/Edmund_Bacon_(architect)\" title=\"Edmund Bacon (architect)\">Edmund Bacon</a>,\n",
       " <a href=\"/wiki/Michael_Bacon_(musician)\" title=\"Michael Bacon (musician)\">Michael Bacon</a>,\n",
       " <a href=\"/wiki/Footloose_(1984_film)\" title=\"Footloose (1984 film)\">Footloose</a>,\n",
       " <a href=\"/wiki/JFK_(film)\" title=\"JFK (film)\">JFK</a>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting Pages\n",
    "\n",
    "Now, we want to follow these references, gather more urls, and repeat. For the sake of not running to exhaustion, I abbreviate the output using only a large length requirement for the link list.  To traverse all the pages we would simply change the \n",
    "\n",
    "```python\n",
    "while len(links) > 100:\n",
    "```\n",
    "\n",
    "to \n",
    "\n",
    "```python\n",
    "while len(links) > 0:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_wikilinks(url):\n",
    "    links = []\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for link in soup.find('div', {'id': 'bodyContent'}).find_all('a', href = re.compile('^(/wiki/)((?!:).)*$')):\n",
    "        links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Stacy_Keach\n",
      "https://en.wikipedia.org/wiki/Old_Yeller-Belly\n",
      "https://en.wikipedia.org/wiki/The_Simpsons_(season_29)\n",
      "https://en.wikipedia.org/wiki/D%27oh!\n",
      "https://en.wikipedia.org/wiki/TV_Land\n",
      "https://en.wikipedia.org/wiki/Nicktoons_(United_States)\n",
      "https://en.wikipedia.org/wiki/Hallmark_Channel\n",
      "https://en.wikipedia.org/wiki/Super_Bowl\n",
      "https://en.wikipedia.org/wiki/New_Haven,_Connecticut\n",
      "https://en.wikipedia.org/wiki/List_of_Connecticut_locations_by_per_capita_income\n",
      "https://en.wikipedia.org/wiki/Vernon,_Connecticut\n",
      "https://en.wikipedia.org/wiki/Pacific_Islander_(U.S._Census)\n",
      "https://en.wikipedia.org/wiki/Hispanic_and_Latino_Americans\n",
      "https://en.wikipedia.org/wiki/Chicano\n",
      "https://en.wikipedia.org/wiki/List_of_U.S._communities_with_Hispanic_majority_populations_in_the_2010_census\n",
      "https://en.wikipedia.org/wiki/Kendale_Lakes,_Florida\n",
      "https://en.wikipedia.org/wiki/Native_Alaskan\n",
      "https://en.wikipedia.org/wiki/2010_United_States_Census\n",
      "https://en.wikipedia.org/wiki/Washington-Baltimore-Arlington,_DC-MD-VA-WV-PA_Combined_Statistical_Area\n",
      "https://en.wikipedia.org/wiki/List_of_mayors_of_Washington,_D.C.\n",
      "https://en.wikipedia.org/wiki/List_of_Governors_of_Missouri\n",
      "https://en.wikipedia.org/wiki/Lilburn_W._Boggs\n",
      "https://en.wikipedia.org/wiki/William_Henry_Ashley\n",
      "https://en.wikipedia.org/wiki/Peter_Kinder\n",
      "https://en.wikipedia.org/wiki/Missouri_gubernatorial_election,_2012\n",
      "https://en.wikipedia.org/wiki/United_States_presidential_election_in_Missouri,_1968\n",
      "https://en.wikipedia.org/wiki/Missouri_gubernatorial_election,_1896\n",
      "https://en.wikipedia.org/wiki/St._Louis,_Missouri\n",
      "https://en.wikipedia.org/wiki/Spain\n",
      "https://en.wikipedia.org/wiki/Roman_era\n",
      "https://en.wikipedia.org/wiki/Theophilos_(emperor)\n",
      "https://en.wikipedia.org/wiki/Thekla,_wife_of_Michael_II\n"
     ]
    }
   ],
   "source": [
    "links = get_wikilinks('https://en.wikipedia.org/wiki/Kevin_Bacon')\n",
    "while len(links) > 100:\n",
    "    newArticle = 'https://en.wikipedia.org' + links[random.randint(0, len(links)-1)].attrs['href']\n",
    "    print(newArticle)\n",
    "    links = get_wikilinks(newArticle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "Write a function to retrieve a list of albums of any area you are interested in using Wikipedia's list of list of albums page: https://en.wikipedia.org/wiki/Lists_of_albums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
